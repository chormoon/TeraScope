---
title: "report"
author: "Yang Cong"
date: "2022/1/20"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(fig.align = "center")
```

```{r load, include=FALSE}
# Source the code from analysis script in src folder
 library('ProjectTemplate')
 library(hms)
 library(dplyr)
 library(tidyr)
 library(ggplot2)
 load.project()
 source("src/eda.R")
```
# **Introduction**
## **Background**
#### Terapixel images offer an intuitive, accessible way to present information sets to stakeholders, allowing viewers to interactively browse big data across multiple scales. The challenge we addressed here is how to deliver the supercomputer scale resources needed to compute a realistic terapixel visualization of the city of Newcastle upon Tyne and its environmental data as captured by the Newcastle Urban Observatory.

#### Our solution is a scalable architecture for cloud-based visualization that we can deploy and pay for only as needed. The three key objectives of this work are to: create a supercomputer architecture for scalable visualization using the public cloud; produce a terapixel 3D city visualization supporting daily updates; undertake a rigorous evaluation of cloud supercomputing for compute intensive visualization applications.

#### We demonstrate that it is feasible to produce a high quality terapixel visualization using a path tracing renderer in under a day using public IaaS cloud GPU nodes. Once generated the terapixel image supports interactive browsing of the city and its data at a range of sensing scales from the whole city to a single desk in a room, accessible across a wide range of thin client devices.


## **Need for the project**
#### The dataset was created from application checkpoint and system metric output from the production of a terapixel image. There are varieties of problems that can be addressed using the TeraScope dataset. What we need is to use what we learned to explore and analyse the Data.

# **Explore and analyse**
## **Data Process**
#### First, I decide to clean and deduplicate the data. Then look at the first question, that is, 'Which event types dominate task runtimes?'. To solve this question, I count the hostname for each data set and check whether both data set the hostname is the same using the setequal function of dplyr.
#### To get which event dominate task runtimes, we first should create a new data frame to compute the task runtimes and change time to hms format. I also create a new feature called duration which features the running time for each event. The new data frame we finally get is bellow.
```{r echo=FALSE}
head(DurationsNew)
```

#### Then, for the next question: 'What is the interplay between GPU temperature and performance?' We only need the powerDrawWatt gpuTempC gpuUtilPerc and gpuMemUtilPerc. Here we creat three data frames to find out there interplay.
```{r echo=FALSE}
head(data_power_temp)
head(data_gpuUtilPerc_temp)
head(data_gpuMemUtilPerc_temp)
```
#### For question: 'What is the interplay between increased power draw and render time?' We need to find the relationship between power and render time. So I firstly calculate the avg render for each hostname. Then I calculate the avg power for each hostname. Now we can combine the data as bellow.
```{r echo=FALSE}
head(avg_power_render)
```
#### For quesion: 'Can we identify particular GPU cards (based on their serial numbers) whose performance differs to other cards? (i.e. perpetually slow cards)' We should calculate average Task Duration for Each GPU Seria and draw the total render time avg for each host. During this we also need to find out the relation between these differnt data frame to find a correct way to combine them. By checking it is obviously each hostname is a gpu card. So we can continue our calculating and combining. Here I finally draw 10 data so that it can be easier to analysis.
```{r echo=FALSE}
head(host_card)
```
#### Finally the last quesion: 'What can we learn about the efficiency of the task scheduling process?' First we should find out which can be uesd to measure the efficiency. I think the efficiency would be measured by the amount of idle time between tasks. For this analysis we would assess each hostname separately and we would record the total idle time for each hostname. The data would be used to analyse is below.
```{r echo=FALSE}
head(efficiency)
```


## **Data Analysis**
#### 